from pathlib import Path

import click
from mlflow.tracking import MlflowClient

from .logger import logger
from .tracker import (
    MLFLOW_TRACKING_URI_DESCRIPTION,
    get_metrics_from_run_id,
    mlflow_default_tracking_uri,
    tag_run,
)
from .train import DEFAULT_OUTPUT_FOLDER as DEFAULT_TRAINING_OUTPUT_FOLDER
from .types import EvaluationResult
from .utils import (
    load_model_training_run_id,
    read_json_from_file,
    save_evaluation_result,
)

DEFAULT_EVALUATION_OUTPUT_FOLDER = Path("output") / "evaluation"
EVALUATION_OUTPUT_FILE = "result.json"


def compare_metrics(
    training_metrics: dict, threshold_metrics: dict
) -> EvaluationResult:
    """Compare metrics from training with given thresholds.

    Args:
        training_metrics (dict):  Metrics generated by training
        threshold_metrics (dict): Minimum threshold values for each metric

    Returns:
        EvaluationResult:
            Evaluation result including information whether evaluation passed or failed
    """
    for key, value in threshold_metrics.items():
        if key not in training_metrics or training_metrics[key] < value:
            logger.info(f"Metric {key} failed")
            return {"passed": False}
    return {"passed": True}


def _read_threshold_metrics(file_path: Path) -> dict:
    try:
        return read_json_from_file(Path(file_path))
    except Exception:
        logger.exception("Failed reading threshold metrics")
        raise


def run_evaluate(
    training_output_dir: Path,
    output_dir: Path,
    threshold_metrics_file_path: Path,
    mlflow_tracking_uri: str,
) -> None:
    logger.info(f"Reading training output from {training_output_dir}")
    run_id = load_model_training_run_id(
        training_output_dir=training_output_dir
    )
    client = MlflowClient(tracking_uri=mlflow_tracking_uri)
    training_metrics = get_metrics_from_run_id(run_id=run_id, client=client)

    logger.info(f"Using threshold metrics file: {threshold_metrics_file_path}")
    threshold_metrics = _read_threshold_metrics(threshold_metrics_file_path)

    evaluation_result = compare_metrics(training_metrics, threshold_metrics)
    logger.info(f"Evaluation result: {evaluation_result}")

    logger.info(f"Saving evaluation outputs to {output_dir}")
    evaluation_output_file = output_dir / EVALUATION_OUTPUT_FILE
    save_evaluation_result(evaluation_result, evaluation_output_file)

    tag_run(evaluation_result, run_id, client)


@click.command()
@click.option(
    "--training-output-dir",
    type=str,
    default=DEFAULT_TRAINING_OUTPUT_FOLDER,
    help="Path to the folder containing the training outcomes",
)
@click.option(
    "--threshold-metrics-file",
    default="conf/threshold_metrics_for_evaluation.json",
    type=str,
    help="Path to the file containing the threshold values of the evaluation metrics",
)
@click.option(
    "--mlflow-tracking-uri",
    type=str,
    default=mlflow_default_tracking_uri(),
    help=MLFLOW_TRACKING_URI_DESCRIPTION,
)
@click.option(
    "--output-dir",
    "-o",
    default=DEFAULT_EVALUATION_OUTPUT_FOLDER,
    help="Path to the folder containing the evaluation outcomes",
)
def cli(
    training_output_dir: str,
    threshold_metrics_file: str,
    mlflow_tracking_uri: str,
    output_dir: str,
):
    """
    This script reads the evaluation metrics generated during training (stored in
    the tracking server) and compares them with the thresholds values defined in the
    'threshold-metrics-file' in the '/conf' directory.

    Customize the metrics in this file if needed.
    """

    training_output_dir = Path(training_output_dir).resolve()
    output_dir = Path(output_dir).resolve()
    threshold_metrics_file_path = Path(threshold_metrics_file).resolve()

    run_evaluate(
        training_output_dir=training_output_dir,
        output_dir=output_dir,
        mlflow_tracking_uri=mlflow_tracking_uri,
        threshold_metrics_file_path=threshold_metrics_file_path,
    )


if __name__ == "__main__":
    cli()
