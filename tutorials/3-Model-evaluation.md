<h1>  Model Evaluation </h1>

This readme guides you on how to work with evaluating model in our pipeline.

[TOC]

## Purpose of the evaluation step

The evaluation step is introduced after the execution of the training step. During the training step evaluation metrics have been generated and stored in MLFlow UI. Those evaluation metrics should meet some particular threshold values and this is the purpose of this step.

The purpose of the Evaluation step is to make sure that the model is performing better than defined thresholds.

## How the evaluation component is working

Basic idea behind this component is to obtain the metric results of an experiment and to compare them against defined threshold values.

The model evaluation step is defined in [training/evaluate.py](../training/evaluate.py).

For achieving the above the inputs to the evaluation component are:

1. a folder path containing a file generated at the training step (we refer to this as input file).
   This file contains the MLFLOW run_id that was created during the execution of the training step.

2. [a configuration file](../conf/threshold_metrics_for_evaluation.json) where we are saving the evaluation metrics that we want our model to pass

The evaluation component is extracting the `run_id` from the input file and connects to MLFlow tracking server from where it reads the evaluation metrics that are generated for that particular experiment.
After that the evaluation component compares those metrics with the metrics that are stored in the configuration file.

If the evaluation metric that is described in the configuration file is not met, then the evaluation is considered as failed.

## Output of the evaluation component

Evaluation component returns a path to a folder where a file named `result.json` with evaluation results is stored.
This folder is an input to the register component.
The register component reads the file generated by the evaluation step and decides to register the model or not.

The code implemented in the evaluation step also places a tag in the `run_id` of that experiment informing if the evaluation for that particular `run_id` has **passed** or **failed**.

## Limitations

For this evaluation to work we need to have a match between the name of the evaluation metrics saved in MLFlow tracking server and the name used in configuration file.
Internally we have done a name convention and we are using the prefix `eval_` to tag the metrics that we are going to evaluate.

Right now our comparison metrics logic implemented in the [`evaluate.py`](../training/evaluate.py) supports comparisons where the actual evaluation metrics are approved if they are greater than a threshold.

In the example evaluation configuration file [`threshold_metrics_for_evaluation.json`](../conf/threshold_metrics_for_evaluation.json), we defined a threshold value for the accuracy metric of our model.

```
{
    "eval_acc": 0.7
}
```

## How to customize the code and introduce different evaluation metrics

In order to define your own evaluation metrics, you should first generate them during the training step and store them in MLFlow tracking system. In our example repo the evaluation metrics are generated in the [model.py](../src/training/model.py) file and in the `evaluate_model` function.

For the purpose of identifying to which metrics we should apply our threshold values, we add the prefix `eval_` to those metrics when we save them to the MLFlow UI. The saving of the metrics happens to the [tracker file](../training/tracker.py) and in the function `save_metrics`.

After that, the next step should be to update the [evaluation configuration file](../conf/threshold_metrics_for_evaluation.json) and add the corresponding threshold values.
